# summerizer
Machine Learning models designed to generate concise and coherent summaries of longer texts. There are several approaches to text summarization, and I'll briefly outline two main types: extractive summarization and abstractive summarization.

This repo focus is on abstractive summarization. Abstractive summarization goes beyond extractive methods by generating new sentences that capture the original text's meaning but may not appear verbatim. This approach requires a deeper understanding of the content and the ability to generate human-like language.
Abstractive summarization models often use neural networks, such as recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and transformer models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer).

In this repo, I build different models for summarization purposes. Generally, the libraries are based on JAX, Trask, PyTorch, and Tensorflow. For each project, you can find the dependencies in the `requirment.txt` file. The list of projects is as follows:

1. Transformer Decoder (GPT-2).





