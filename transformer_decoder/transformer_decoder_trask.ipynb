{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","import os\n","\n","import time\n","import numpy as np\n","import gin\n","\n","import textwrap\n","wrapper = textwrap.TextWrapper(width=70)\n","\n","import trax\n","from trax import layers as tl\n","from trax.fastmath import numpy as jnp\n","\n","# to print the entire np array\n","np.set_printoptions(threshold=sys.maxsize)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def PositionalEncoder(vocab_size, d_model, dropout, max_len, mode):\n","    \"\"\"Returns a list of layers that: \n","    1. takes a block of text as input, \n","    2. embeds the words in that text, and \n","    3. adds positional encoding, \n","       i.e. associates a number in range(max_len) with \n","       each word in each sentence of embedded input text \n","    \n","    The input is a list of tokenized blocks of text\n","    \n","    Args:\n","        vocab_size (int): vocab size.\n","        d_model (int):  depth of embedding.\n","        dropout (float): dropout rate (how much to drop out).\n","        max_len (int): maximum symbol length for positional encoding.\n","        mode (str): 'train' or 'eval'.\n","    \"\"\"\n","    # Embedding inputs and positional encoder\n","    return [ \n","        # Add embedding layer of dimension (vocab_size, d_model)\n","        tl.Embedding(vocab_size, d_model),  \n","        # Use dropout with rate and mode specified\n","        tl.Dropout(rate=dropout, mode=mode), \n","        # Add positional encoding layer with maximum input length and mode specified\n","        tl.PositionalEncoding(max_len=max_len, mode=mode)] "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def FeedForward(d_model, d_ff, dropout, mode, ff_activation):\n","    \"\"\"Returns a list of layers that implements a feed-forward block.\n","\n","    The input is an activation tensor.\n","\n","    Args:\n","        d_model (int):  depth of embedding.\n","        d_ff (int): depth of feed-forward layer.\n","        dropout (float): dropout rate (how much to drop out).\n","        mode (str): 'train' or 'eval'.\n","        ff_activation (function): the non-linearity in feed-forward layer.\n","\n","    Returns:\n","        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n","    \"\"\"\n","    \n","    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n","    return [ \n","        # Normalize layer inputs\n","        tl.LayerNorm(), \n","        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n","        tl.Dense(d_ff), \n","        # Add activation function passed in as a parameter (you need to call it!)\n","        ff_activation(),  # Generally ReLU\n","        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n","        tl.Dropout(rate=dropout, mode=mode), \n","        # Add second feed forward layer (don't forget to set the correct value for n_units)\n","        tl.Dense(d_model), \n","        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n","        tl.Dropout(rate=dropout, mode=mode) \n","    ]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def DecoderBlock(d_model, d_ff, n_heads,\n","                 dropout, mode, ff_activation):\n","    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n","\n","    The input is an activation tensor.\n","\n","    Args:\n","        d_model (int):  depth of embedding.\n","        d_ff (int): depth of feed-forward layer.\n","        n_heads (int): number of attention heads.\n","        dropout (float): dropout rate (how much to drop out).\n","        mode (str): 'train' or 'eval'.\n","        ff_activation (function): the non-linearity in feed-forward layer.\n","\n","    Returns:\n","        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n","    \"\"\"\n","        \n","    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n","    return [\n","      tl.Residual(\n","          # Normalize layer input\n","          tl.LayerNorm(), \n","          # Add causal attention \n","          tl.CausalAttention(d_model, n_heads=n_heads, dropout=dropout, mode=mode) \n","        ),\n","      tl.Residual(\n","          # Add feed-forward block\n","          # We don't need to normalize the layer inputs here. The feed-forward block takes care of that for us.\n","          FeedForward(d_model, d_ff, dropout, mode, ff_activation)\n","        ),\n","      ]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def TransformerLM(vocab_size=33300,\n","                  d_model=512,\n","                  d_ff=2048,\n","                  n_layers=6,\n","                  n_heads=8,\n","                  dropout=0.1,\n","                  max_len=4096,\n","                  mode='train',\n","                  ff_activation=tl.Relu):\n","    \"\"\"Returns a Transformer language model.\n","\n","    The input to the model is a tensor of tokens. (This model uses only the\n","    decoder part of the overall Transformer.)\n","\n","    Args:\n","        vocab_size (int): vocab size.\n","        d_model (int):  depth of embedding.\n","        d_ff (int): depth of feed-forward layer.\n","        n_layers (int): number of decoder layers.\n","        n_heads (int): number of attention heads.\n","        dropout (float): dropout rate (how much to drop out).\n","        max_len (int): maximum symbol length for positional encoding.\n","        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n","        ff_activation (function): the non-linearity in feed-forward layer.\n","\n","    Returns:\n","        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n","        to activations over a vocab set.\n","    \"\"\"\n","    \n","    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n","    decoder_blocks = [ \n","        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)] \n","\n","    # Create the complete model as written in the figure\n","    return tl.Serial(\n","        # Use teacher forcing (feed output of previous step to current step)\n","        tl.ShiftRight(mode=mode), \n","        # Add embedding inputs and positional encoder\n","        PositionalEncoder(vocab_size, d_model, dropout, max_len, mode),\n","        # Add decoder blocks\n","        decoder_blocks, \n","        # Normalize layer\n","        tl.LayerNorm(), \n","\n","        # Add dense layer of vocab_size (since need to select a word to translate to)\n","        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n","        tl.Dense(vocab_size), \n","        # Get probabilities with Logsoftmax\n","        tl.LogSoftmax() \n","    )"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
